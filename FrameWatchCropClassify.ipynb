{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user scikit-image\n",
    "#pip install --user keras\n",
    "#pip install --user opencv-python-headless\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Root directory of the Mask_RCNN project\n",
    "ROOT_DIR = os.path.abspath(\"/home/idies/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/Mask_RCNN\")\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import keras.backend\n",
    "import paradim\n",
    "\n",
    "#%matplotlib inline \n",
    "\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "PARADIM_WEIGHTS_PATH = \"/home/idies/workspace/paradim_data/models/summer_school_model.h5\"  # TODO: update this path\n",
    "#PARADIM_WEIGHTS_PATH = \"/home/idies/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/trained_models/mask_rcnn_paradim_0020_bigtrain.h5\" \n",
    "#PARADIM_WEIGHTS_PATH = \"/home/idies/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/Mask_RCNN/logs/paradim20190609T0213/mask_rcnn_paradim_0020.h5\"  # TODO: update this path\n",
    "\n",
    "PARADIM_DIR = \"/home/idies/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/datasets/paradim\"\n",
    "\n",
    "\n",
    "#main\n",
    "import time\n",
    "import os\n",
    "import cv2 #pip install --user opencv-python-headless  \n",
    "#it is important to install the headless version as there is a missing shared lib for the regular opencv-python\n",
    "from SciServer import CasJobs\n",
    "\n",
    "\n",
    "my_context = \"MyDB\"\n",
    "path_prefix = '/home/idies/workspace'\n",
    "to_process_path = '/home/idies/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/datasets/paradim/current/goodMelt/frame.jpg'\n",
    "CROP_DIR = '/home/idies/workspace/paradim_data/frameClass/croppedFrames'\n",
    "CLASSIFIED_DIR = '/home/idies/workspace/paradim_data/frameClass/classifiedFrames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "\n",
    "class ParadimClassifier:\n",
    "    \n",
    "    def classifyCurrent(self, i, classified_frame_path):\n",
    "\n",
    "        image_id = random.choice(self.dataset.image_ids)\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(self.dataset, self.config, image_id, use_mini_mask=False)\n",
    "        info = self.dataset.image_info[image_id]\n",
    "        #print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "        #                               self.dataset.image_reference(image_id)))\n",
    "\n",
    "        # Run object detection\n",
    "        results = self.model.detect([image], verbose=1)\n",
    "\n",
    "        # Display results\n",
    "        \n",
    "        #close prvious results?\n",
    "        plt.close()\n",
    "        \n",
    "        r = results[0]\n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            self.dataset.class_names, r['scores'], #ax=self.ax,\n",
    "                            title=\"Predictions\", path_to_save = classified_frame_path)\n",
    "        \n",
    "\n",
    "        #self.fig.canvas.draw()\n",
    "        #plt.show()\n",
    "        #log(\"gt_class_id\", gt_class_id)\n",
    "        #log(\"gt_bbox\", gt_bbox)\n",
    "        #log(\"gt_mask\", gt_mask)\n",
    "        return r\n",
    "    \n",
    "    def __init__(self, weights_path, dataset_dir, model_dir):\n",
    "        self.config = paradim.ParadimConfig()\n",
    "        # Override the training configurations with a few\n",
    "        # changes for inferencing.\n",
    "        class InferenceConfig(self.config.__class__):\n",
    "            # Run detection on one image at a time\n",
    "            GPU_COUNT = 1\n",
    "            IMAGES_PER_GPU = 1\n",
    "            DETECTION_MIN_CONFIDENCE = .4\n",
    "            IMAGE_RESIZE_MODE=\"square\"\n",
    "\n",
    "        self.config = InferenceConfig()\n",
    "        self.config.display()\n",
    "        # Device to load the neural network on.\n",
    "        DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "        TEST_MODE = \"inference\"\n",
    "\n",
    "        # Load validation dataset\n",
    "        self.dataset = paradim.ParadimDataset()\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.dataset.load_paradim(self.dataset_dir, \"current\")\n",
    "\n",
    "        # Must call before using the dataset\n",
    "        self.dataset.prepare()\n",
    "\n",
    "        #print(\"Images: {}\\nClasses: {}\".format(len(self.dataset.image_ids), self.dataset.class_names))\n",
    "        \n",
    "        #lil hack to make it work as specified in \n",
    "        #https://github.com/matterport/Mask_RCNN/issues/30\n",
    "        K = keras.backend.backend()\n",
    "        if K=='tensorflow':\n",
    "            keras.backend.set_image_dim_ordering('tf')\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        \n",
    "        with tf.device(DEVICE):\n",
    "            self.model = modellib.MaskRCNN(mode=\"inference\", model_dir=self.model_dir, config=self.config)\n",
    "            \n",
    "        self.weights_path = weights_path\n",
    "\n",
    "        # Or, load the last model you trained\n",
    "        #weights_path = model.find_last()\n",
    "\n",
    "        # Load weights\n",
    "        print(\"Loading weights \", self.weights_path)\n",
    "        self.model.load_weights(self.weights_path, by_name=True)\n",
    "        \n",
    "        #self.fig, self.ax = plt.subplots(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.4\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  576\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  432\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [576 576   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_bbox_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           paradim\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           64\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "Loading weights  /home/idies/workspace/paradim_data/models/summer_school_model.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer #391 (named \"mrcnn_bbox_fc\"), weight <tf.Variable 'mrcnn_bbox_fc_2/kernel:0' shape=(1024, 16) dtype=float32_ref> has shape (1024, 16), but the saved weight has shape (1024, 12).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-afeb591f1c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParadimClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARADIM_WEIGHTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARADIM_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-09829e0baf2a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights_path, dataset_dir, model_dir)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m#self.fig, self.ax = plt.subplots(1,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Storage/ncarey/persistent/PARADIM/furnace_ml/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                                          \u001b[0;34m' has shape {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                                          \u001b[0;34m', but the saved weight has shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m                                          str(weight_values[i].shape) + '.')\n\u001b[0m\u001b[1;32m   1150\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     weight_value_tuples.append((symbolic_weights[i],\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #391 (named \"mrcnn_bbox_fc\"), weight <tf.Variable 'mrcnn_bbox_fc_2/kernel:0' shape=(1024, 16) dtype=float32_ref> has shape (1024, 16), but the saved weight has shape (1024, 12)."
     ]
    }
   ],
   "source": [
    "def frameCrop(path, hotseat_path, crop_path):\n",
    "    xStartPct = .65 \n",
    "    xStopPct = 0.95\n",
    "    yStartPct = .1\n",
    "    yStopPct = .5\n",
    "    \n",
    "    img = cv2.imread(path)\n",
    "    width, height, pixels = img.shape\n",
    "    crop_img = img[(int)(width*yStartPct):(int)(width*yStopPct),(int)(height*xStartPct):(int)(height*xStopPct)].copy()\n",
    "    cv2.imwrite(crop_path, crop_img)\n",
    "    cv2.imwrite(hotseat_path, crop_img)\n",
    "\n",
    "\n",
    "classifier = ParadimClassifier(weights_path = PARADIM_WEIGHTS_PATH, dataset_dir = PARADIM_DIR, model_dir = MODEL_DIR)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for unprocessed...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0b7ccc4e0dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#start classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifyCurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassified_frame_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#update DB stating that the image has been processed (change processed from 0 to 1 in relevant row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while True: #really should be while true. Will change in future, I just didnt want it running indefinitely waiting for a CTRL+C\n",
    "    count = count + 1\n",
    "    #time.sleep(.1)\n",
    "    \n",
    "    checkup_query = \"select * from FramePathProcessing where processed = 0 ORDER BY Id\"\n",
    "    print(\"searching for unprocessed...\")\n",
    "    unprocessed_df = CasJobs.executeQuery(sql=checkup_query, context=my_context)\n",
    "    if len(unprocessed_df['Path']) == 0:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    for unprocessed_path in unprocessed_df['Path']: \n",
    "        full_path = os.path.join(path_prefix, unprocessed_path)\n",
    "        base_name = os.path.basename(full_path)\n",
    "        crop_path = os.path.join(CROP_DIR, base_name)\n",
    "        frameCrop(full_path, to_process_path, crop_path)\n",
    "        classified_frame_path = os.path.join(CLASSIFIED_DIR, base_name)\n",
    "        \n",
    "        \n",
    "        #start classification\n",
    "        r = classifier.classifyCurrent(count, classified_frame_path)\n",
    "        #update DB stating that the image has been processed (change processed from 0 to 1 in relevant row)\n",
    "        #print(r)\n",
    "        print(r['scores'])\n",
    "        goodMeltScore = 0\n",
    "        fastBottomScore = 0\n",
    "        fastTopScore = 0\n",
    "        for index in range(len(r['class_ids'])):\n",
    "            cur_class_id = r['class_ids'][index]\n",
    "            cur_class_score = r['scores'][index]\n",
    "            #need to fix multiple scores of same class...\n",
    "            if cur_class_id == 1:\n",
    "                goodMeltScore = cur_class_score\n",
    "            elif cur_class_id == 2:\n",
    "                fastBottomScore = cur_class_score\n",
    "            elif cur_class_id == 3:\n",
    "                fastTopScore = cur_class_score\n",
    "            else:\n",
    "                print(\"ERROR, UNRECOGNIZED CLASS ID FROM MR_CNN CLASSIFIER\")\n",
    "            \n",
    "        # TODO this would be faster if i insert via ID instead of Path\n",
    "        update_sql = '''UPDATE FramePathProcessing\n",
    "                        SET processed = 1, goodMeltScore = {0}, fastBottomScore = {1}, fastTopScore = {2}, processedPath = '{3}', cropPath = '{4}'\n",
    "                        WHERE Path = '{5}' '''.format(goodMeltScore, fastBottomScore, fastTopScore, classified_frame_path, crop_path, unprocessed_path)\n",
    "        print(update_sql)\n",
    "        CasJobs.executeQuery(sql=update_sql, context=my_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUGGING, DONT RUN THIS\n",
    "\n",
    "checkup_query = \"select * from FramePathProcessingFirstRun where processed = 0 ORDER BY Id\"\n",
    "print(\"searching for unprocessed...\")\n",
    "unprocessed_df = CasJobs.executeQuery(sql=checkup_query, context=my_context)\n",
    "count = 1\n",
    "for unprocessed_path in unprocessed_df['Path'][6:]: \n",
    "    full_path = os.path.join(path_prefix, unprocessed_path)\n",
    "    base_name = os.path.basename(full_path)\n",
    "    crop_path = os.path.join(CROP_DIR, base_name)\n",
    "    frameCrop(full_path, to_process_path, crop_path)\n",
    "    classified_frame_path = os.path.join(CLASSIFIED_DIR, base_name)\n",
    "        \n",
    "        \n",
    "    #start classification\n",
    "    r = classifier.classifyCurrent(count, classified_frame_path)\n",
    "    #update DB stating that the image has been processed (change processed from 0 to 1 in relevant row)\n",
    "    #print(r)\n",
    "    print(classified_frame_path)\n",
    "    print(r['scores'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
